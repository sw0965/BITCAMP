05/20

DNN = ANN

DNN에서 파생 DNN -> CNN, 
             DNN -> RNN

CNN = Convolutional Neural Network
Convolutional = 복잡한
-합성곱 신경망 

RNN = Recurrent Neural Network    시계열
Recurrent = 반복되는
RNN에 대표 LSTM
-LSTM = Long short Term Memory

스칼라 벡터 행렬 텐서 가장 중요한거 앞으로 할거

스칼라 1개

[[1,2,3]],[1,2,3]]             (2,3)   행렬
[[1,2],[4,3]],[[4,5],[5,6]]    (2,2,2) 3차원 텐서
[[[1],[2],[3]],[[4],[5],[6]]]  (2,3,1)
[[[1,2,3,4]]]                  (1,1,4)
[[[[1],[2]]],[[[3],[4]]]]      (2,1,2,1)

망각 인풋 셀 아웃풋 -> lstm 요소 4가지 


-------------------------------------------GRU---------------------------------------------------------
조경현 박사님이 제안한 구조
LSTM과 유사하게 생겼는데, LSTM을 더 간략화한 구조
hidden state만 흘러가고 cell state는 없음
Update gate는 이번 step에서 계산한 hidden을 얼마나 update할지 결정한다. (update 되는만큼 기존의 정보를 잊는다.)
LSTM의 forget, input gate를 하나의 Update gate로!
만약 z가 0이라면 이번 step의 히든 스테이트는 이전 레이어의 히든 스테이트를 그대로 Copy합니다(identity mapping)

RNN은 게이트가 하나