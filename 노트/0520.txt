05/20

DNN = ANN

DNN에서 파생 DNN -> CNN, 
             DNN -> RNN

CNN = Convolutional Neural Network
Convolutional = 복잡한
-합성곱 신경망 

RNN = Recurrent Neural Network    시계열
Recurrent = 반복되는
RNN에 대표 LSTM
-LSTM = Long short Term Memory

스칼라 벡터 행렬 텐서 가장 중요한거 앞으로 할거

스칼라 1개

[[1,2,3]],[1,2,3]]             (2,3)   행렬
[[1,2],[4,3]],[[4,5],[5,6]]    (2,2,2) 3차원 텐서
[[[1],[2],[3]],[[4],[5],[6]]]  (2,3,1)
[[[1,2,3,4]]]                  (1,1,4)
[[[[1],[2]]],[[[3],[4]]]]      (2,1,2,1)

망각 인풋 셀 아웃풋 -> lstm 요소 4가지 


-------------------------------------------GRU---------------------------------------------------------
조경현 박사님이 제안한 구조
LSTM과 유사하게 생겼는데, LSTM을 더 간략화한 구조
hidden state만 흘러가고 cell state는 없음
Update gate는 이번 step에서 계산한 hidden을 얼마나 update할지 결정한다. (update 되는만큼 기존의 정보를 잊는다.)
LSTM의 forget, input gate를 하나의 Update gate로!
만약 z가 0이라면 이번 step의 히든 스테이트는 이전 레이어의 히든 스테이트를 그대로 Copy합니다(identity mapping)

RNN은 게이트가 하나



0522 ctr+f5할때 나오는것들만 데이터 정리



두번째 lstm 명확하진않지만 순차적데이터 

38번경우 x=11 가정하에

MinMaxScaler:  정규화
         x - min(x)
x1=----------------------
      max(x) - min(x)


모든 feature가 0과 1사이에 위치하게 만듭니다.
데이터가 2차원 셋일 경우, 모든 데이터는 x축의 0과 1 사이에, y축의 0과 1사이에 위치하게 됩니다.

from sklearn.preprocessing import MinMaxScaler

standscaler: 표준화
          x - 평균
x1 = ----------------
         표준편차
각 feature의 평균을 0, 분산을 1로 변경합니다. 모든 특성들이 같은 스케일을 갖게 됩니다.

minmax : 최솟값 0 최댓값 1 
1,2,3,5,10


stand = x = 1,2,3,4,10 일때

평균 = 4
편차 (평균-값들)= 3, 2, 1, 0, -6
분산(편차를 제곱) = 9, 4, 1, 0, 36 = 50/5 = 10
표준편차 = 루트10 


전처리는 x 만하면됌 y는 결괏값이니 할 필요가 없다 . 모델에서 값을 좋게 만드는것이니.